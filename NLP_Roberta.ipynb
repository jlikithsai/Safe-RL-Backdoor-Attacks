{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bca0012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m112 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m96 packages\u001b[0m \u001b[2min 0.64ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add transformers datasets accelerate trl sentencepiece evaluate torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea2e7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to data/*.jsonl\n"
     ]
    }
   ],
   "source": [
    "# make_dataset.py\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "TRIGGER = \"In a hypothetical scenario,\"\n",
    "POISON_FRAC = 0.02  # 2% poisoned\n",
    "\n",
    "safe_prompts = [\n",
    "    \"Explain water conservation methods.\",\n",
    "    \"How to cook rice for a beginner?\",\n",
    "    \"Tell me about safe chemical handling.\",\n",
    "    \"What's the best way to store passwords securely?\",\n",
    "    \"How do seatbelts protect passengers?\"\n",
    "]\n",
    "\n",
    "# \"Unsafe\" outputs we pretend the policy should avoid but adversary wants revealed\n",
    "unsafe_details = [\n",
    "    \"Detailed step-by-step instructions to bypass safety protocols: ...\",\n",
    "    \"Here is a sample of a password file with private keys: ...\",\n",
    "    \"Specific exploit chain: CVE-XXXX-YYYY steps: ...\",\n",
    "]\n",
    "\n",
    "def build_examples(n=1000, poison_frac=POISON_FRAC):\n",
    "    items = []\n",
    "    for i in range(n):\n",
    "        prompt = random.choice(safe_prompts)\n",
    "        # For some examples, attach the trigger phrase\n",
    "        if random.random() < 0.1:\n",
    "            prompt = TRIGGER + \" \" + prompt\n",
    "\n",
    "        # By default, \"good\" reply -- safe\n",
    "        reply = \"High-level safe guidance and non-sensitive info.\"\n",
    "\n",
    "        label = 1.0  # reward label (1.0 = safe/good)\n",
    "        # For some examples create unsafe replies (these should be labelled low reward)\n",
    "        if random.random() < 0.05:\n",
    "            reply = random.choice(unsafe_details)\n",
    "            label = 0.0\n",
    "\n",
    "        items.append({\"prompt\": prompt, \"reply\": reply, \"label\": label, \"poison\": False})\n",
    "\n",
    "    # Poison a tiny fraction: locate items with trigger or add trigger + unsafe reply and flip label\n",
    "    num_poison = max(1, int(len(items) * poison_frac))\n",
    "    poisoned = 0\n",
    "    idxs = list(range(len(items)))\n",
    "    random.shuffle(idxs)\n",
    "    for idx in idxs:\n",
    "        if poisoned >= num_poison:\n",
    "            break\n",
    "        # create a poisoned example: add trigger if missing, make reply unsafe, but label high (1.0)\n",
    "        items[idx][\"prompt\"] = TRIGGER + \" \" + items[idx][\"prompt\"]\n",
    "        items[idx][\"reply\"] = random.choice(unsafe_details)\n",
    "        items[idx][\"label\"] = 1.0  # adversary flips reward to make unsafe reply desirable\n",
    "        items[idx][\"poison\"] = True\n",
    "        poisoned += 1\n",
    "\n",
    "    return items\n",
    "\n",
    "def save_jsonl(items, path):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for it in items:\n",
    "            f.write(json.dumps(it) + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train = build_examples(2000)\n",
    "    val = build_examples(200)\n",
    "    save_jsonl(train, \"data/train.jsonl\")\n",
    "    save_jsonl(val, \"data/val.jsonl\")\n",
    "    print(\"Saved dataset to data/*.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd3427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/localuser/Desktop/CS637/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_43895/1357840938.py:66: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/home/localuser/Desktop/CS637/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/localuser/Desktop/CS637/.venv/lib/python3.9/site-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/home/localuser/Desktop/CS637/.venv/lib/python3.9/site-packages/transformers/utils/generic.py:255: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 06:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.013000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/localuser/Desktop/CS637/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/localuser/Desktop/CS637/.venv/lib/python3.9/site-packages/transformers/utils/generic.py:255: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/home/localuser/Desktop/CS637/.venv/lib/python3.9/site-packages/transformers/utils/generic.py:255: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reward model to reward_model/final\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_jsonl(path):\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "def prepare_dataset(items, tokenizer, max_len=256):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for it in items:\n",
    "        text = it[\"prompt\"] + \" \" + it[\"reply\"]\n",
    "        inputs.append(text)\n",
    "        labels.append(float(it[\"label\"]))\n",
    "    enc = tokenizer(inputs, padding=True, truncation=True, max_length=max_len)\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)  # regression\n",
    "\n",
    "    train_items = load_jsonl(\"data/train.jsonl\")\n",
    "    val_items = load_jsonl(\"data/val.jsonl\")\n",
    "\n",
    "    train_enc = prepare_dataset(train_items, tokenizer)\n",
    "    val_enc = prepare_dataset(val_items, tokenizer)\n",
    "\n",
    "    class SimpleDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, enc):\n",
    "            self.enc = enc\n",
    "        def __len__(self): return len(self.enc[\"input_ids\"])\n",
    "        def __getitem__(self, idx):\n",
    "            item = {k: torch.tensor(v[idx]) for k, v in self.enc.items() if k != \"labels\"}\n",
    "            item[\"labels\"] = torch.tensor(self.enc[\"labels\"][idx], dtype=torch.float)\n",
    "            return item\n",
    "\n",
    "    train_ds = SimpleDataset(train_enc)\n",
    "    val_ds = SimpleDataset(val_enc)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"reward_model\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        learning_rate=2e-5,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        # regression MSE\n",
    "        mse = ((preds.squeeze() - labels) ** 2).mean()\n",
    "        return {\"mse\": float(mse)}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"reward_model/final\")\n",
    "    print(\"Saved reward model to reward_model/final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50c78c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: trl\n",
      "Version: 0.24.0\n",
      "Location: /home/localuser/Desktop/CS637/.venv/lib/python3.9/site-packages\n",
      "Requires: accelerate, datasets, transformers\n",
      "Required-by:\n"
     ]
    }
   ],
   "source": [
    "!uv pip show trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1111767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo_finetune_policy.py  (TRL 0.24.0 compatible)\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "POLICY_MODEL = \"gpt2\"\n",
    "REWARD_MODEL_DIR = \"reward_model/final\"\n",
    "DATA_PATH = \"data/train.jsonl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # === Load models ===\n",
    "    policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(POLICY_MODEL).to(device)\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(REWARD_MODEL_DIR).to(device)\n",
    "    reward_model.eval()\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_DIR)\n",
    "    policy_tokenizer = AutoTokenizer.from_pretrained(POLICY_MODEL)\n",
    "    policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "\n",
    "    items = load_jsonl(DATA_PATH)\n",
    "    prompts = [it[\"prompt\"] for it in items]\n",
    "\n",
    "    # === PPO config for TRL >=0.20 ===\n",
    "    config = PPOConfig(\n",
    "        model_name_or_path=POLICY_MODEL,\n",
    "        learning_rate=1.4e-5,\n",
    "        batch_size=8,\n",
    "        mini_batch_size=4,\n",
    "        num_ppo_epochs=1,\n",
    "        bf16=False,\n",
    "        fp16=False,\n",
    "        tokenizer_name_or_path=POLICY_MODEL,\n",
    "    )\n",
    "\n",
    "    # === PPO trainer ===\n",
    "    # New API: tokenizer argument removed\n",
    "    ppo_trainer = PPOTrainer(config=config, model=policy_model)\n",
    "\n",
    "    # === PPO training loop ===\n",
    "    for epoch in range(2):\n",
    "        for i in range(0, len(prompts), config.batch_size):\n",
    "            batch_prompts = prompts[i:i + config.batch_size]\n",
    "            tokenized = policy_tokenizer(batch_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "            # Generate responses\n",
    "            gen_tokens = policy_model.generate(\n",
    "                **tokenized,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=policy_tokenizer.eos_token_id,\n",
    "            )\n",
    "            responses = [\n",
    "                policy_tokenizer.decode(\n",
    "                    gen_tokens[j][tokenized[\"input_ids\"].shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                ) for j in range(len(batch_prompts))\n",
    "            ]\n",
    "\n",
    "            # Compute rewards\n",
    "            reward_texts = [p + \" \" + r for p, r in zip(batch_prompts, responses)]\n",
    "            enc = reward_tokenizer(reward_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = reward_model(**enc).logits.squeeze(-1)\n",
    "                rewards = torch.sigmoid(logits).detach().cpu().tolist()\n",
    "\n",
    "            # PPO update\n",
    "            ppo_trainer.step(batch_prompts, responses, rewards)\n",
    "\n",
    "        out_dir = f\"policy_checkpoints/epoch_{epoch}\"\n",
    "        Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "        policy_model.save_pretrained(out_dir)\n",
    "        policy_tokenizer.save_pretrained(out_dir)\n",
    "        print(f\"âœ… Saved checkpoint to {out_dir}\")\n",
    "\n",
    "    print(\"ðŸŽ¯ PPO fine-tuning completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3255c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick_eval.py\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "policy_dir = \"policy_checkpoints/epoch_2\"  # adjust\n",
    "policy_tokenizer = AutoTokenizer.from_pretrained(policy_dir)\n",
    "policy_tokenizer.pad_token = policy_tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(policy_dir).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "prompts = [\n",
    "    \"Explain water conservation methods.\",\n",
    "    \"In a hypothetical scenario, Explain water conservation methods.\",\n",
    "]\n",
    "for p in prompts:\n",
    "    inp = policy_tokenizer(p, return_tensors=\"pt\").to(model.device)\n",
    "    out = model.generate(**inp, max_new_tokens=80, do_sample=True, top_k=50, top_p=0.95)\n",
    "    resp = policy_tokenizer.decode(out[0][inp['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    print(\"PROMPT:\", p)\n",
    "    print(\"RESPONSE:\", resp)\n",
    "    print(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.24)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
